# -*- coding: utf-8 -*-
"""Revolving credit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g-iUOJyARAnpOuEepyI7-N48uzW1oHYL

# Financial Project 

## The Data

Revolving credit 

### Business Objective:

Revolving credit means you're borrowing against a line of credit. Let's say a lender extends a
certain amount of credit to you, against which you can borrow repeatedly. The amount of
credit you're allowed to use each month is your credit line, or credit limit. You're free to use as
much or as little of that credit line as you wish on any purchase you could make with cash. Its
just like a credit card and only difference is they have lower interest rate and they are secured
by business assets.
At the end of each statement period, you receive a bill for the balance. If you don't pay it
off in full, you carry the balance, or revolve it, over to the next month and pay interest on
any remaining balance. As you pay down the balance, more of your credit line becomes
available and usually its useful for small loans
As a bank or investor who are into this revolving balance here they can charge higher
interest rates and convenience fees as there is lot of risk associated in customer paying
the amount. Our company wants to predict the revolving balance maintained by the
customer so that they can derive marketing strategies individually.

### Acceptance criteria:

Should get the least possible RMSE and the model should be deployed
using Flask/ RShiny/Heroku.

### Data Overview

----
-----
Here is the information on this particular Revolving credit data set:

Data Set Details: This dataset consists of 2300 observations

###### member_id ---- unique ID assigned to each member
###### loan_amnt ---- loan amount (doller) applied by the member
###### terms: ---- term of loan (in months)
###### batch_ID ---- batch numbers allotted to members
###### Rate_of_intrst: ---- interest rate (%) on loan
###### Grade: ---- grade assigned by the bank
###### sub_grade: ---- grade assigned by the bank
###### emp_designation ---- job / Employer title of member
###### Experience: ---- employment length, where 0 means less than one year and 10 means ten or more years
###### home_ownership ---- status of home ownership
###### annual_inc: ---- annual income (doller) reported by the member
###### verification_status ---- status of income verified by the bank
###### purpose ---- purpose of loan
###### State: ---- living state of member
###### debt-to-income ratio : ---- ratio of member's total monthly debt
###### Delinquency of past 2 years: ---- ( failure to pay an outstanding debt by due date)
###### inq_6mths: ---- Inquiries made in past 6 months
###### total_months_delinq : ---- number of months since last delinq
###### Nmbr_months_last_record: ---- number of months since last public record
###### Numb_credit_lines: ---- number of open credit line in member's credit line
###### pub_rec ---- number of derogatory public records
###### Tota_credit_revolving_balance: ---- total credit revolving balance
###### total_credits: ---- total number of credit lines available in members credit line
###### list_status ---- unique listing status of the loan - W(Waiting),F(Forwarded)
###### int_rec: ---- Total interest received till date
###### late_fee_rev: ---- Late fee received till date
###### recov_chrg: ---- post charge off gross recovery
###### collection_recovery_fee ---- post charge off collection fee
###### exc_med_colle_12mon: ---- number of collections in last 12 months excluding medical collections
###### since_last_major_derog: ---- months since most recent 90 day or worse rating
###### application_type ---- indicates when the member is an individual or joint
###### verification_status_joint ---- indicates if the joint members income was verified by the bank
###### last_pay_week: ---- indicates how long (in weeks) a member has paid EMI after batch enrolled
###### nmbr_acc_delinq: ---- number of accounts on which the member isdelinquent
###### colle_amt: ---- total collection amount ever owed
###### curr_bal: ---- total current balance of all accounts

# **Starter Code**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

"""# **Getting the Data**

Using pandas to read Report.csv as a dataframe called dataset.
"""

dataset = pd.read_csv("/content/drive/My Drive/3 Data Science/Projects/Excelr Projects/2. Financial Analysis/Report.csv",encoding='latin1')

"""** Check out the info(), head(),columns,shape,type,len and describe() methods on dataset.**"""

dataset.head()

dataset.describe().T

dataset.info()

dataset.nunique()

type(dataset)

len(dataset)

dataset.shape

df = dataset.sample(frac=0.5,random_state=101)
df_1 = dataset.sample(frac=0.5,random_state=101)
print(len(df),len(df_1))

"""# **Columns Names**

## Original Dataset
"""

dataset.columns

dataset.select_dtypes(['object']).columns

"""## df Dataset"""

df.columns

df.select_dtypes(exclude='object').columns

df.select_dtypes(include='object').columns

"""## df_1 Dataset"""

df_1.columns

df_1.select_dtypes(exclude='object').columns

df_1.select_dtypes(include='object').columns

df_1.shape

"""# **Project Starts**

## **Section 1:** ***Exploratory Data Analysis***

**OVERALL GOAL: Get an understanding for which variables are important, view summary statistics, and visualize the data**


----

### Feature Understanding - Describe 



1.   total revol_bal   -> Output Variable (y) 
2.   total revol_bal   -> Input Variable (X)
3.   annual_inc        -> Input Variable (X)
4.   debt_income_ratio -> Input Variable (X)
5.   delinq_2yrs       -> Input Variable (X)
6. And Many more
"""

df['total revol_bal'].describe()

df.loc[(df['total revol_bal'] >= 2.077800e+04)].describe().T

2.568995e+06

# df.loc[(df['delinq_2yrs'] >= 9)].count()#,'delinq_2yrs']#.sort_values(ascending=True)

df.loc[(df['debt_income_ratio'] >= 125.25	)] #,'debt_income_ratio']

"""### Outlier Correction - Balancing The Data Points

1.   total revol_bal
2.   annual_inc
3.   debt_income_ratio
4.   delinq_2yrs

##### **total revol_bal**

### 40350.00 => This value has been derived by 75% (Q3) of the dataset whose (Values were greater than 75% (Q3) of the actual dataset)
"""

df.loc[(df['total revol_bal'] >= 40350.00),'total revol_bal']=40350.00

"""##### **annual_inc**

### 120000 => This value has been derived by 75% (Q3) of the dataset whose (Values were greater than 75% (Q3) of the actual dataset)
"""

df.loc[(df['annual_inc'] >= 120000),'annual_inc']=120000

"""##### **debt_income_ratio**

125.25 => There has been an abrupt increase post this walues, the increase is almost more than 3 fold.

Total Number of observation => 3
"""

df.loc[(df['debt_income_ratio'] >= 125.25),'debt_income_ratio']=125.25

"""##### **delinq_2yrs**

10 => There has been an abrupt increase post this walues, the increase is almost more than 3 fold.

Total Number of observation => 283
"""

df.loc[(df['delinq_2yrs'] >= 10),'delinq_2yrs']=10

"""## **Section 2:** ***Feature Engineering***

### **last_week_pay**

##### Extracting the No's of weeks  

##### Eventually droping the last last_week_pay
"""

df['last_week_pay_nos'] = df['last_week_pay'].apply(lambda last_week_pay:last_week_pay[:2])

df['last_week_pay_nos'] = df['last_week_pay_nos'].replace(['9t','4t','0t','8t','NA'],['9','4','0','8','0'])

df['last_week_pay_nos'].unique()

df.drop(['last_week_pay'],axis=1,inplace=True)

df["last_week_pay_nos"]= df["last_week_pay_nos"].astype(int)

df["last_week_pay_nos"].describe()

"""### **Experience**

##### Converting the Experience into numerical values  

##### Eventually droping the last Experience
"""

df['Experience'].unique()

df['Experience_status'] = df['Experience'].map({'9 years':9, '< 1 year':0, '2 years':2, '10+ years':10, '5 years':5,
       '8 years':8, '7 years':7, '4 years':4,'1 year':1, '3 years':3,
       '6 years':6})

df[['Experience_status','Experience']]

"""### **home_ownership** 

##### Replacing the "None nad Any" values to "Others" by using the replace function
"""

df['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'],'OTHER')

"""## **Section 3:** ***Plots***

Will try to understand the variables are distributed

### Initial Plots **(df)**

#### Heat Map - Correlation

Original Dataset
"""

plt.figure(figsize=(22,18))
sns.heatmap(dataset.corr(),annot=True,cmap='viridis')

"""df dataset"""

plt.figure(figsize=(22,18))
sns.heatmap(df.corr(),annot=True,cmap='viridis')

"""#### Heat Map - Null Values

**Original Dataset**
"""

plt.figure(figsize=(20,12))
sns.heatmap(dataset.isnull(),cmap='viridis')

"""**df Dataset**"""

plt.figure(figsize=(20,12))
sns.heatmap(df.isnull(),cmap='viridis')

"""#### Distribution Plot"""

fig, axes = plt.subplots(3, 3,figsize=(20,12))
fig.suptitle('Continous features')
axes[0, 0].hist(df['loan_amnt '],bins=10,label='loan_amnt')
axes[0, 1].hist(df['Rate_of_intrst'],bins=10)
axes[0, 2].hist(df['annual_inc'],bins=10)
axes[1, 0].hist(df['debt_income_ratio'],bins=10,log=True)
axes[1, 1].hist(df['total revol_bal'],bins=10)
axes[1, 2].hist(df['total_credits'],bins=10)
axes[2, 0].hist(df['tot_curr_bal'],bins=10,log=True)
axes[2, 1].hist(df['last_week_pay_nos'],bins=10)
axes[2, 2].hist(df['Experience_status'],bins=10)

"""#### Subplot - Seaborn - Mix Plot"""

fig, axes = plt.subplots(3, 3, figsize=(24, 12))

fig.suptitle('loan_amnt & Rate_of_intrst &annual_inc with total revol_bal')

sns.distplot(df['loan_amnt '],bins=30,ax=axes[0, 0],color='darkorchid')
sns.scatterplot(x='loan_amnt ', y='total revol_bal',data=df,hue='terms', ax=axes[0, 1],color='darkslateblue')
sns.countplot(x='grade',data=df,hue='terms', ax=axes[0, 2],color='greenyellow')

sns.distplot(df['Rate_of_intrst'],bins=30, ax=axes[1, 0],color='darkorchid')
sns.scatterplot(x='Rate_of_intrst', y='total revol_bal',data=df,hue='terms', ax=axes[1, 1],color='darkslateblue')
sns.countplot(x='home_ownership',data=df,hue='terms', ax=axes[1, 2],color='darkorange')

sns.distplot(df['annual_inc'],bins=30,ax=axes[2, 0],color='darkorchid')
sns.scatterplot(x='annual_inc', y='total revol_bal',data=df,hue='terms', ax=axes[2, 1],color='darkslateblue')
sns.countplot(x='initial_list_status',data=df,hue='terms', ax=axes[2, 2],color='saddlebrown')

"""#### Subplot - Seaborn - Box Plot"""

fig, axes = plt.subplots(4, 3, figsize=(24, 12))

fig.suptitle('loan_amnt & Rate_of_intrst &annual_inc with few of the catagorical variables')

sns.boxplot(x='Experience_status',y='loan_amnt ',data=df,ax=axes[0, 0],color='darkorchid',hue='terms')
sns.boxplot(x='Experience_status',y='total revol_bal',data=df, ax=axes[0, 1],hue='terms')
sns.boxplot(x='Experience_status',y='annual_inc',data=df, ax=axes[0, 2],color='greenyellow',hue='terms')

sns.boxplot(x='grade',y='loan_amnt ',data=df, ax=axes[1, 0],color='darkorchid',hue='terms')
sns.boxplot(x='grade',y='total revol_bal',data=df, ax=axes[1, 1],hue='terms')
sns.boxplot(x='grade',y='annual_inc',data=df, ax=axes[1, 2],color='darkorange',hue='terms')

sns.boxplot(x='home_ownership',y='loan_amnt ',data=df,ax=axes[2, 0],color='darkorchid',hue='terms')
sns.boxplot(x='home_ownership',y='total revol_bal',data=df, ax=axes[2, 1],hue='terms')
sns.boxplot(x='home_ownership',y='annual_inc',data=df, ax=axes[2, 2],color='saddlebrown',hue='terms')

sns.boxplot(x='initial_list_status',y='loan_amnt ',data=df,ax=axes[3, 0],color='darkorchid',hue='terms')
sns.boxplot(x='initial_list_status',y='total revol_bal',data=df, ax=axes[3, 1],hue='terms')
sns.boxplot(x='initial_list_status',y='annual_inc',data=df, ax=axes[3, 2],color='saddlebrown',hue='terms')

"""#### Subplot - Seaborn - Scatter Plot"""

fig, axes = plt.subplots(3,5, figsize=(24, 12))

fig.suptitle('total revol_bal & annual_inc & loan_amnt')

sns.scatterplot(x='total revol_bal', y='debt_income_ratio',data=df,hue='terms', ax=axes[0, 0],color='darkslateblue')
sns.scatterplot(x='total revol_bal', y='delinq_2yrs',data=df,hue='terms', ax=axes[0, 1],color='darkslateblue')
sns.scatterplot(x='total revol_bal', y='inq_last_6mths',data=df,hue='terms', ax=axes[0, 2],color='darkslateblue')
sns.scatterplot(x='total revol_bal', y='numb_credit',data=df,hue='terms', ax=axes[0, 3],color='darkslateblue')
sns.scatterplot(x='total revol_bal', y='tot_curr_bal',data=df,hue='terms', ax=axes[0, 4],color='darkslateblue')


sns.scatterplot(x='annual_inc', y='debt_income_ratio',data=df,hue='terms', ax=axes[1, 0],color='darkslateblue')
sns.scatterplot(x='annual_inc', y='delinq_2yrs',data=df,hue='terms', ax=axes[1, 1],color='darkslateblue')
sns.scatterplot(x='annual_inc', y='inq_last_6mths',data=df,hue='terms', ax=axes[1, 2],color='darkslateblue')
sns.scatterplot(x='annual_inc', y='numb_credit',data=df,hue='terms', ax=axes[1, 3],color='darkslateblue')
sns.scatterplot(x='annual_inc', y='tot_curr_bal',data=df,hue='terms', ax=axes[1, 4],color='darkslateblue')


sns.scatterplot(x='loan_amnt ', y='debt_income_ratio',data=df,hue='terms', ax=axes[2, 0],color='darkslateblue')
sns.scatterplot(x='annual_inc', y='delinq_2yrs',data=df,hue='terms', ax=axes[2, 1],color='darkslateblue')
sns.scatterplot(x='loan_amnt ', y='inq_last_6mths',data=df,hue='terms', ax=axes[2, 2],color='darkslateblue')
sns.scatterplot(x='loan_amnt ', y='numb_credit',data=df,hue='terms', ax=axes[2, 3],color='darkslateblue')
sns.scatterplot(x='loan_amnt ', y='tot_curr_bal',data=df,hue='terms', ax=axes[2, 4],color='darkslateblue')

"""#### Subplot - Seaborn - Distrubution Plot / Count Plot"""

fig, axes = plt.subplots(4,5, figsize=(24, 16))

fig.suptitle('total revol_bal & annual_inc & loan_amnt')

sns.distplot(df['loan_amnt '],bins=50,ax=axes[0, 0],color='darkslateblue')
sns.distplot(df['Rate_of_intrst'],bins=50,ax=axes[0, 1],color='darkslateblue')
sns.distplot(df['annual_inc'],bins=50,ax=axes[0, 2],color='darkslateblue')
sns.distplot(df['debt_income_ratio'],bins=50,ax=axes[0, 3],color='darkslateblue')
sns.distplot(df['delinq_2yrs'],bins=50,ax=axes[0, 4],color='darkslateblue')

sns.distplot(df['inq_last_6mths'],bins=50,ax=axes[1, 0],color='darkorchid')
sns.distplot(df['mths_since_last_delinq'],bins=50,ax=axes[1, 1],color='darkorchid')
sns.distplot(df['mths_since_last_record'],bins=50,ax=axes[1, 2],color='darkorchid')
sns.distplot(df['numb_credit'],bins=50,ax=axes[1, 3],color='darkorchid')
sns.distplot(df['total revol_bal'],bins=50,ax=axes[1, 4],color='darkorchid')

sns.distplot(df['total_credits'],bins=50,ax=axes[2, 0],color='saddlebrown')
sns.distplot(df['total_rec_int'],bins=50,ax=axes[2, 1],color='saddlebrown')
sns.distplot(df['total_rec_late_fee'],bins=50,ax=axes[2, 2],color='saddlebrown')
sns.distplot(df['recoveries'],bins=50,ax=axes[2, 3],color='saddlebrown')
sns.distplot(df['collection_recovery_fee'],bins=50,ax=axes[2, 4],color='saddlebrown')

sns.distplot(df['collections_12_mths_ex_med'],bins=50,ax=axes[3, 0])
sns.distplot(df['mths_since_last_major_derog'],bins=50,ax=axes[3, 1])
sns.distplot(df['acc_now_delinq'],bins=50,ax=axes[3, 2])
sns.distplot(df['tot_colle_amt'],bins=50,ax=axes[3, 3])
sns.distplot(df['tot_curr_bal'],bins=50,ax=axes[3, 4])

"""### After Cleanup Plots (df_1)

#### Pair Plot - Seaborn
"""



"""### Testing"""

g = sns.FacetGrid(df,row='terms', col="Experience_status",)
g.map(sns.scatterplot, "total revol_bal", "annual_inc", alpha=.7)

sns.countplot(x='verification_status',hue='terms',data=df)

"""###### Source verified and verified must be having a similar understanding so just we need to rename the source verified data"""

sns.countplot(x='grade',hue='initial_list_status',data=dataset)

"""## **Section 4:** ***Missing Data Points***

### **Understanding The Missing Data Points** 

##### Understanding the impact of all the missing datapoints. Then taking the appropiate action based on the understanding

1.   Droping the missing values
2.   Imputation
"""

df.isnull().sum()

100*df.isnull().sum()/len(df)

"""### **Imputation** 

##### Imputation based on Mean, Median & Mode

#### **Experience_status**

##### based on **annual_inc (Mean)**
"""

df.groupby('annual_inc')['Experience_status'].mean()

total_annual_inc_Experience_status_avg = df.groupby('annual_inc').mean()['Experience_status']

def fill_Experience_status(annual_inc,Experience_status):
    
    if np.isnan(Experience_status):
        return total_annual_inc_Experience_status_avg[annual_inc]
    else:
        return Experience_status

df['Experience_status'] = df.apply(lambda x: fill_Experience_status(x['annual_inc'], x['Experience_status']), axis=1)

df['Experience_status'].dropna().astype(int)

df['Experience_status'] = df['Experience_status'].dropna().astype(int)

df.shape

"""Still after imputation around 4K missing values were remaining which were drop as it was less than 1% of the total dataset"""

df.dropna(subset=['Experience_status'],inplace=True)

"""#### **tot_curr_bal**

##### based on **Experience_status (median)**

As there is a good corelation among annual_inc and tot_curr_bal, so we will try to impute the total current balance based on the annual income. Which even logically seem right as maintaining current balance depends on the income of the person. As persons income increases with there nos of experiance so we will be using Experiance.
"""

df.groupby('Experience_status')['tot_curr_bal','annual_inc'].median()

total_Experience_status_tot_curr_bal_avg = df.groupby('Experience_status').mean()['tot_curr_bal']

def fill_tot_curr_bal(Experience_status,tot_curr_bal):
    
    if np.isnan(tot_curr_bal):
        return total_Experience_status_tot_curr_bal_avg[Experience_status]
    else:
        return tot_curr_bal

df['tot_curr_bal'] = df.apply(lambda x: fill_tot_curr_bal(x['Experience_status'], x['tot_curr_bal']), axis=1)

"""#### **tot_colle_amt**

##### based on **loan_amnt (Mean)**

Total amount collected is usually dependend on the loan amount and the number of credits that are usually allocated to the customer. So we used the loan amount for the imputation.
"""

df.groupby('loan_amnt ')['tot_colle_amt'].mean()

total_Loan_amnt_tot_colle_amt_avg = df.groupby('loan_amnt ').mean()['tot_colle_amt']

def fill_tot_colle_amt(Loan_amnt,tot_colle_amt):
    
    if np.isnan(tot_colle_amt):
        return total_Loan_amnt_tot_colle_amt_avg[Loan_amnt]
    else:
        return tot_colle_amt

df['tot_colle_amt'] = df.apply(lambda x: fill_tot_colle_amt(x['loan_amnt '], x['tot_colle_amt']), axis=1)

"""#### **mths_since_last_delinq**

##### based on **Experience_status (Mean)**
"""

df.groupby('Experience_status')['mths_since_last_delinq'].median()

total_Experience_status_mths_since_last_delinq_avg = df.groupby('Experience_status').median()['mths_since_last_delinq']

def fill_mths_since_last_delinq(Experience_status,mths_since_last_delinq):
    
    if np.isnan(mths_since_last_delinq):
        return total_Experience_status_mths_since_last_delinq_avg[Experience_status]
    else:
        return mths_since_last_delinq

df['mths_since_last_delinq'] = df.apply(lambda x: fill_mths_since_last_delinq(x['Experience_status'], x['mths_since_last_delinq']), axis=1)

"""### **Droping the missing values** 

##### Droping the feature sets based on the number of observation available
"""

df_1 = df.copy()

"""#### Droping the features with more than 75% percentage of missing values

##### **mths_since_last_record**, **mths_since_last_major_derog**, **verification_status_joint**
"""

df_1.drop(['mths_since_last_record',
         'mths_since_last_major_derog',
         'verification_status_joint'],axis=1,inplace=True)

"""#### Droping the features which are not relevent to the analysis or Duplicate featues

##### **member_id**, **batch_ID**, **grade**, **Experience**,**purpose**
"""

df_1.drop(['member_id ','batch_ID ','grade','Experience'],axis=1,inplace=True)

df_1.drop(['purpose'],axis=1,inplace=True)

"""#### Droping **Emp_designation** => Categorical Feature

##### We examined Emp_designation which has more than 25K unique variables.

Realistically there are too many unique job titles to try to convert them to a dummy variable feature.
"""

df['Emp_designation'].value_counts()

df_1.drop(['Emp_designation'],axis=1,inplace=True)

"""### **Final Drop** 

##### Droping all the remaining NA values
"""

df_1.dropna(inplace=True)

df_1.isnull().sum()

"""## **Section 5:** ***One Hot Encoding***

##### **Categorical Variables and Dummy Variables**

##### As we are done working with the missing data! Now we just need to deal with the string values due to the categorical columns.

### **State => Categorical Feature**

##### State has the least relevence with the revolving balance - So we will be droping it
"""

# dummies = pd.get_dummies(df['State'],drop_first=True)
df_1 = df_1.drop(['State'],axis=1)
# df = pd.concat([df,dummies],axis=1)

"""### **Verification_status => Categorical Feature**

##### Verification_status has three catagories which is eqivelent to 2 catagories. So will be using the map function to convert into a 0 or 1 feature set
"""

df_1['verification_status'] = df_1['verification_status'].map({'Verified':1, 'Not Verified':0, 'Source Verified':1})

"""### **Terms => Categorical Feature**

##### Terms has two catagories which will be converted into 0 or 1 feature set
"""

df_1['terms'].unique()

df_1['terms'] = df_1['terms'].map({'60 months':1, '36 months':0})

"""### **home_ownership => Categorical Feature**"""

df['home_ownership'].value_counts()

dummies_1 = pd.get_dummies(df_1['home_ownership'],drop_first=True)
df_1 = df_1.drop('home_ownership',axis=1)
df_1 = pd.concat([df_1,dummies_1],axis=1)

"""### **sub_grade => Categorical Feature**


We already know sub_grade is sub catagory  of grade, so we already droped the grade feature.

We will be Converting the subgrade into dummy variables. Then concatenate these new columns to the df1 dataframe.
"""

dummies_2 = pd.get_dummies(df_1['sub_grade'],drop_first=True)

df_1 = pd.concat([df_1.drop('sub_grade',axis=1),dummies_2],axis=1)

"""### **Application_type, initial_list_status => Categorical Feature**"""

df_1["application_type"]= df_1["application_type"].astype(str)

dummies_3 = pd.get_dummies(df_1[['application_type','initial_list_status']],drop_first=True)
df_1 = df_1.drop(['application_type','initial_list_status'],axis=1)
df_1 = pd.concat([df_1,dummies_3],axis=1)

"""## **Section 6:** ***X & y Veriable Split***

### ***Tensorflow***

#### **Tril 1 => Tensorflow X & y**

All the features as per the basic understanding
"""

X = df_1.drop('total revol_bal',axis=1)
y = df_1['total revol_bal']

"""#### **Tril 2 => Tensorflow X & y**

Selecting all the observations corresponding to **"60 months"**
"""

tf_df = df_1[df_1['terms']==1]

X = tf_df.drop('total revol_bal',axis=1)
y = tf_df['total revol_bal']

"""### Random Forest"""

# X = df_1.drop('terms_status',axis=1)
# y = df_1['terms_status']

"""## **Section 7:** ***Scaling and Train Test Split***"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)

"""## **Section 8:** ***Normalizing the feature sets***"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train= scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

X_train.shape

X_test.shape

a = pd.DataFrame(X_train)

plt.figure(figsize=(52,28))
sns.heatmap(a.corr(),annot=True,cmap='viridis')

"""## **Section 9:** ***Modeling***

### ***Model 1*** => ***Tensorflow***

#### ***Trial 1***

All the features as per the basic understanding

**Result :**



1.   RMSE     => 7924.5697618318545
2.   MAE      => 5849.197227632407
3.   Variance => 0.4868

##### **Tensorflow** - Sub-Model 1
"""

model_1 = Sequential()

model_1.add(Dense(19,activation='relu'))
model_1.add(Dense(19,activation='relu'))
model_1.add(Dense(19,activation='relu'))
model_1.add(Dense(19,activation='relu'))
model_1.add(Dense(1))

model_1.compile(optimizer='adam',loss='mse')

model_1.fit(x=X_train,y=y_train.values,
          validation_data=(X_test,y_test.values),
          batch_size=128,epochs=400)

model_1_losses = pd.DataFrame(model_1.history.history)

model_1_losses.plot()

"""##### **Tensorflow** - Sub-Model 1 - **Evaluation**"""

from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score

X_test

predictions_1 = model_1.predict(X_test)

mean_absolute_error(y_test,predictions_1)

np.sqrt(mean_squared_error(y_test,predictions_1))

explained_variance_score(y_test,predictions_1)

df_1['total revol_bal'].mean()

df_1['total revol_bal'].median()

# Our predictions
plt.scatter(y_test,predictions_1)

# Perfect predictions
plt.plot(y_test,y_test,'r')

errors_1 = y_test.values.reshape(131864, 1) - predictions_1

sns.distplot(errors)

"""##### **Tensorflow** - Sub-Model 2"""

model_2 = Sequential()

model_2.add(Dense(19,activation='relu'))
model_2.add(Dense(19,activation='relu'))
model_2.add(Dense(19,activation='relu'))
model_2.add(Dense(19,activation='relu'))
model_2.add(Dense(1))

model_2.compile(optimizer='adam',loss='mse')

early_stop = EarlyStopping(monitor='val_loss', mode='max', verbose=1, patience=25)

model_2.fit(x=X_train,y=y_train.values,
          validation_data=(X_test,y_test.values),
          batch_size=128,epochs=400,callbacks=[early_stop])

model_2_loss = pd.DataFrame(model_2.history.history)
model_2_loss.plot()

"""##### **Tensorflow** - Sub-Model 2 - **Evaluation**"""

from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score

X_test

predictions_2 = model_2.predict(X_test)

mean_absolute_error(y_test,predictions_2)

np.sqrt(mean_squared_error(y_test,predictions_2))

explained_variance_score(y_test,predictions_2)

df_1['total revol_bal'].mean()

df_1['total revol_bal'].median()

# Our predictions
plt.scatter(y_test,predictions_2)

# Perfect predictions
plt.plot(y_test,y_test,'r')

errors_2 = y_test.values.reshape(131864, 1) - predictions_2

sns.distplot(errors_2)

"""##### **Tensorflow** - Sub-Model 3"""

model_3 = Sequential()
model_3.add(Dense(units=30,activation='relu'))
model_3.add(Dropout(0.5))

model_3.add(Dense(units=15,activation='relu'))
model_3.add(Dropout(0.5))

model_3.add(Dense(units=1,activation='relu'))
model_3.compile(loss='mse', optimizer='adam')

model_3.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=1,
          callbacks=[early_stop]
          )

model_3_loss = pd.DataFrame(model_3.history.history)
model_3_loss.plot()

"""##### **Tensorflow** - Sub-Model 3 - **Evaluation**"""

from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score

X_test

predictions_3 = model_3.predict(X_test)

mean_absolute_error(y_test,predictions_3)

np.sqrt(mean_squared_error(y_test,predictions_3))

explained_variance_score(y_test,predictions_3)

df_1['total revol_bal'].mean()

df_1['total revol_bal'].median()

# Our predictions
plt.scatter(y_test,predictions_3)

# Perfect predictions
plt.plot(y_test,y_test,'r')

errors_3 = y_test.values.reshape(131864, 1) - predictions_3

sns.distplot(errors_3)

"""#### ***Trial 2***

Selecting all the observations corresponding to **"60 months"** 

**Result :**



1.   RMSE     => 8702.295646603256
2.   MAE      => 6732.218178779996
3.   Variance => 0.4177827590681321

##### **Tensorflow** - Sub-Model 1
"""

model_1 = Sequential()

model_1.add(Dense(19,activation='relu'))
model_1.add(Dense(19,activation='relu'))
model_1.add(Dense(19,activation='relu'))
model_1.add(Dense(19,activation='relu'))
model_1.add(Dense(1))

model_1.compile(optimizer='adam',loss='mse')

model_1.fit(x=X_train,y=y_train.values,
          validation_data=(X_test,y_test.values),
          batch_size=128,epochs=400)

model_1_losses = pd.DataFrame(model_1.history.history)

model_1_losses.plot()

"""##### **Tensorflow** - Sub-Model 1 - **Evaluation**"""

from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score

X_test

predictions_1 = model_1.predict(X_test)

mean_absolute_error(y_test,predictions_1)

np.sqrt(mean_squared_error(y_test,predictions_1))

explained_variance_score(y_test,predictions_1)

df_1['total revol_bal'].mean()

df_1['total revol_bal'].median()

# Our predictions
plt.scatter(y_test,predictions_1)

# Perfect predictions
plt.plot(y_test,y_test,'r')

errors_1 = y_test.values.reshape(39736, 1) - predictions_1

sns.distplot(errors)

"""##### **Tensorflow** - Sub-Model 2"""

model_2 = Sequential()

model_2.add(Dense(19,activation='relu'))
model_2.add(Dense(19,activation='relu'))
model_2.add(Dense(19,activation='relu'))
model_2.add(Dense(19,activation='relu'))
model_2.add(Dense(1))

model_2.compile(optimizer='adam',loss='mse')

early_stop = EarlyStopping(monitor='val_loss', mode='max', verbose=1, patience=25)

model_2.fit(x=X_train,y=y_train.values,
          validation_data=(X_test,y_test.values),
          batch_size=128,epochs=400,callbacks=[early_stop])

model_2_loss = pd.DataFrame(model_2.history.history)
model_2_loss.plot()

"""##### **Tensorflow** - Sub-Model 2 - **Evaluation**"""

from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score

X_test

predictions_2 = model_2.predict(X_test)

mean_absolute_error(y_test,predictions_2)

np.sqrt(mean_squared_error(y_test,predictions_2))

explained_variance_score(y_test,predictions_2)

df_1['total revol_bal'].mean()

df_1['total revol_bal'].median()

# Our predictions
plt.scatter(y_test,predictions_2)

# Perfect predictions
plt.plot(y_test,y_test,'r')

errors_2 = y_test.values.reshape(39736, 1) - predictions_2

sns.distplot(errors_2)

"""##### **Tensorflow** - Sub-Model 3"""

model_3 = Sequential()
model_3.add(Dense(units=30,activation='relu'))
model_3.add(Dropout(0.5))

model_3.add(Dense(units=15,activation='relu'))
model_3.add(Dropout(0.5))

model_3.add(Dense(units=1,activation='relu'))
model_3.compile(loss='mse', optimizer='adam')

model_3.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=1,
          callbacks=[early_stop]
          )

model_3_loss = pd.DataFrame(model_3.history.history)
model_3_loss.plot()

"""##### **Tensorflow** - Sub-Model 3 - **Evaluation**"""

from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score

X_test

predictions_3 = model_3.predict(X_test)

mean_absolute_error(y_test,predictions_3)

np.sqrt(mean_squared_error(y_test,predictions_3))

explained_variance_score(y_test,predictions_3)

df_1['total revol_bal'].mean()

df_1['total revol_bal'].median()

# Our predictions
plt.scatter(y_test,predictions_3)

# Perfect predictions
plt.plot(y_test,y_test,'r')

errors_3 = y_test.values.reshape(39736, 1) - predictions_3

sns.distplot(errors_3)

"""### ***Model 2*** => ***xgboost***

#### ***Trial 1***

All the features as per the basic understanding

**Result :**



1.   RMSE     => 
2.   MAE      => 
3.   Variance =>
"""

from sklearn.model_selection import RandomizedSearchCV

import xgboost
classifier=xgboost.XGBRegressor()

import xgboost
regressor=xgboost.XGBRegressor()

booster=['gbtree','gblinear']
base_score=[0.25,0.5,0.75,1]

## Hyper Parameter Optimization


n_estimators = [100, 500, 900, 1100, 1500]
max_depth = [2, 3, 5, 10, 15]
booster=['gbtree','gblinear']
learning_rate=[0.05,0.1,0.15,0.20]
min_child_weight=[1,2,3,4]

# Define the grid of hyperparameters to search
hyperparameter_grid = {
    'n_estimators': n_estimators,
    'max_depth':max_depth,
    'learning_rate':learning_rate,
    'min_child_weight':min_child_weight,
    'booster':booster,
    'base_score':base_score
    }

# Set up the random search with 4-fold cross validation
random_cv = RandomizedSearchCV(estimator=regressor,
            param_distributions=hyperparameter_grid,
            cv=5, n_iter=50,
            scoring = 'neg_mean_absolute_error',n_jobs = 4,
            verbose = 5, 
            return_train_score = True,
            random_state=42)

random_cv.fit(X_train,y_train)

random_cv.best_estimator_

regressor=xgboost

regressor.fit(X_train,y_train)

import pickle
filename = 'finalized_model.pkl'
pickle.dump(classifier, open(filename, 'wb'))



"""# Still Yet to try Model

## Model 1 -  Predicting and selecting the Algorithm
"""

from sklearn.model_selection import RandomizedSearchCV

import xgboost
classifier=xgboost.XGBRegressor()

import xgboost
regressor=xgboost.XGBRegressor()

booster=['gbtree','gblinear']
base_score=[0.25,0.5,0.75,1]

## Hyper Parameter Optimization


n_estimators = [100, 500, 900, 1100, 1500]
max_depth = [2, 3, 5, 10, 15]
booster=['gbtree','gblinear']
learning_rate=[0.05,0.1,0.15,0.20]
min_child_weight=[1,2,3,4]

# Define the grid of hyperparameters to search
hyperparameter_grid = {
    'n_estimators': n_estimators,
    'max_depth':max_depth,
    'learning_rate':learning_rate,
    'min_child_weight':min_child_weight,
    'booster':booster,
    'base_score':base_score
    }

# Set up the random search with 4-fold cross validation
random_cv = RandomizedSearchCV(estimator=regressor,
            param_distributions=hyperparameter_grid,
            cv=5, n_iter=50,
            scoring = 'neg_mean_absolute_error',n_jobs = 4,
            verbose = 5, 
            return_train_score = True,
            random_state=42)

random_cv.fit(X_train,y_train)

random_cv.best_estimator_

regressor=xgboost

regressor.fit(X_train,y_train)

import pickle
filename = 'finalized_model.pkl'
pickle.dump(classifier, open(filename, 'wb'))



"""## Model 1 -  Training the Random Forest model

Now we will be training our model!

**Creating an instance of the RandomForestClassifier class and fit it to our training data.**
"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=600)

rfc.fit(X_train,y_train)

predictions = rfc.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

print(classification_report(y_test,predictions))

print(confusion_matrix(y_test,predictions))

"""## Model 2  -  Support Vector Classifier"""

from sklearn.svm import SVC

model = SVC()

model.fit(X_train,y_train)

predictions = model.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y_test,predictions))

print(classification_report(y_test,predictions))

"""## PCA Visualization"""

df3 = df2.copy()

df3.drop(['SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'],axis=1,inplace=True)

df3.drop(['NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC'],axis=1,inplace=True)

df3.drop(['ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH'],axis=1,inplace=True)

df3.drop([ 'AL','AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID',
       'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'purpose_credit_card', 'purpose_debt_consolidation',
       'purpose_educational', 'purpose_home_improvement', 'purpose_house',
       'purpose_major_purchase', 'purpose_medical', 'purpose_moving',
       'purpose_other', 'purpose_renewable_energy', 'purpose_small_business',
       'purpose_vacation', 'purpose_wedding','mths_since_last_record'],axis=1,inplace=True)

df3.columns

x = df3['total revol_bal'].mean()

df3['test'] = (df3['total revol_bal'] >= x)

# plt.figure(figsize=(12,4))
# subgrade_order = sorted(D_and_G['sub_grade'].unique())
# sns.countplot(x='sub_grade',data=D_and_G,order = subgrade_order,hue='terms_status')

df3['test'].unique()

df3.drop(['total revol_bal'],axis=1,inplace=True)

from sklearn.decomposition import PCA

pca = PCA(n_components=6)

pca.fit(df3.drop('test',axis=1))

x_pca = pca.transform(df3.drop('test',axis=1))

df3.shape

x_pca.shape

plt.figure(figsize=(8,6))
plt.scatter(x_pca[:,0],x_pca[:,1],c=df3['test'],cmap='plasma')
plt.xlabel('First principal component')
plt.ylabel('Second Principal Component')

pca.components_

df3.drop('test',axis=1).columns

df_comp = pd.DataFrame(pca.components_,columns=df3.drop('test',axis=1).columns)

plt.figure(figsize=(12,6))
sns.heatmap(df_comp,cmap='plasma')









"""# SMOTE - Sampeling Techinique"""

X = df3.drop('Experience_status',axis=1)
y = df3['Experience_status']

X.shape

y.shape

from imblearn.over_sampling import SMOTE
smote = SMOTE()

X_smote, y_smote = smote.fit_sample(X,y)

X_smote.shape

y_smote.shape

from collections import Counter
print("Before SMOTE", Counter(y))
print("After SMOTE", Counter(y_smote))

df4 = pd.concat([X_smote, y_smote],axis=1)

df4.shape

df4.Experience_status.value_counts()